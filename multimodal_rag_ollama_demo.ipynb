{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e53146",
   "metadata": {},
   "source": [
    "# Multimodal RAG with RAG-Anything and Local Ollama\n",
    "\n",
    "This notebook demonstrates how to build a multimodal Retrieval-Augmented Generation (RAG) system using the RAG-Anything library integrated with local Ollama models. We'll cover:\n",
    "\n",
    "- Setting up the environment with RAG-Anything and Ollama\n",
    "- Configuring custom model functions for text LLM, vision, and embeddings\n",
    "- Processing multimodal documents (text + images)\n",
    "- Performing queries with and without vision enhancement\n",
    "\n",
    "The system uses local Ollama models for privacy and cost-effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of RAG-Anything with all extras\n",
    "!pip install rag-anything[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9fdb0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have Ollama installed and running locally on your machine.\n",
    "\n",
    "1. Install Ollama from https://ollama.ai/\n",
    "2. Start the Ollama service\n",
    "3. Pull the required models:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "ollama pull llava\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "Note: If `qwen3-vl` is available and preferred over `llava`, you can use that instead for the vision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import requests\n",
    "import base64\n",
    "from rag_anything import RAGAnything, RAGAnythingConfig\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d75d62",
   "metadata": {},
   "source": [
    "## Ollama Utility Functions\n",
    "\n",
    "These functions provide a simple interface to interact with local Ollama models via HTTP API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama utility functions\n",
    "def chat_with_ollama(prompt, model=\"llama3.2\", stream=False):\n",
    "    \"\"\"Send a chat prompt to Ollama and return the response.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": model, \"prompt\": prompt, \"stream\": stream},\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error communicating with Ollama: {e}\")\n",
    "\n",
    "def embed_with_ollama(text, model=\"nomic-embed-text\"):\n",
    "    \"\"\"Generate embeddings for text using Ollama.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/embeddings\",\n",
    "            json={\"model\": model, \"prompt\": text},\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"embedding\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error getting embeddings from Ollama: {e}\")\n",
    "\n",
    "def vision_with_ollama(image_path, prompt, model=\"llava\"):\n",
    "    \"\"\"Send an image and prompt to Ollama vision model.\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_data = f.read()\n",
    "        image_b64 = base64.b64encode(image_data).decode('utf-8')\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\"model\": model, \"prompt\": prompt, \"images\": [image_b64], \"stream\": False},\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(f\"Image file not found: {image_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Error communicating with Ollama vision model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa909a92",
   "metadata": {},
   "source": [
    "## Custom Function Definitions\n",
    "\n",
    "These functions wrap the Ollama utilities to match the RAG-Anything API requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f925880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model functions for RAG-Anything\n",
    "def llm_model_func(prompt):\n",
    "    \"\"\"Text LLM function using Ollama llama3.2.\"\"\"\n",
    "    return chat_with_ollama(prompt, model=\"llama3.2\")\n",
    "\n",
    "def vision_model_func(image_path, prompt):\n",
    "    \"\"\"Vision model function using Ollama llava.\"\"\"\n",
    "    return vision_with_ollama(image_path, prompt, model=\"llava\")\n",
    "\n",
    "def embedding_func(text):\n",
    "    \"\"\"Embedding function using Ollama nomic-embed-text.\"\"\"\n",
    "    return embed_with_ollama(text, model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943e8e4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the RAG-Anything configuration with our custom model functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Anything configuration\n",
    "config = RAGAnythingConfig(\n",
    "    llm_model_func=llm_model_func,\n",
    "    vision_model_func=vision_model_func,\n",
    "    embedding_func=embedding_func,\n",
    "    embedding_dim=768  # nomic-embed-text produces 768-dimensional embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb075ae",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Create the RAG-Anything instance with the configured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a61d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG-Anything\n",
    "rag = RAGAnything(config)\n",
    "print(\"RAG-Anything initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb36f83",
   "metadata": {},
   "source": [
    "## Document Processing Example\n",
    "\n",
    "Process a sample PDF document. Make sure you have a PDF file available (e.g., 'sample.pdf' in the current directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing example\n",
    "# Replace 'sample.pdf' with the path to your PDF document\n",
    "document_path = \"sample.pdf\"\n",
    "\n",
    "if os.path.exists(document_path):\n",
    "    try:\n",
    "        # Process the document (this may take some time for large documents)\n",
    "        rag.process_document(document_path)\n",
    "        print(f\"Document '{document_path}' processed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "else:\n",
    "    print(f\"Document '{document_path}' not found. Please provide a valid PDF path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9136c5",
   "metadata": {},
   "source": [
    "## Querying Examples\n",
    "\n",
    "Demonstrate different types of queries: text-only and vision-enhanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying examples\n",
    "\n",
    "# Text query\n",
    "text_query = \"What is the main topic of this document?\"\n",
    "try:\n",
    "    text_result = rag.query(text_query)\n",
    "    print(\"Text Query Result:\")\n",
    "    print(text_result)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with text query: {e}\")\n",
    "\n",
    "# Vision-enhanced query (requires an image file)\n",
    "image_path = \"sample_image.jpg\"  # Replace with actual image path\n",
    "vision_query = \"Describe what you see in this image and how it relates to the document content.\"\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    try:\n",
    "        vision_result = rag.query_with_vision(vision_query, image_path=image_path)\n",
    "        print(\"Vision-Enhanced Query Result:\")\n",
    "        print(vision_result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with vision query: {e}\")\n",
    "else:\n",
    "    print(f\"Image '{image_path}' not found. Skipping vision query example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc534d8",
   "metadata": {},
   "source": [
    "## Results Display\n",
    "\n",
    "The results from the queries are displayed above. In a real application, you might want to format these results better or integrate them into a user interface.\n",
    "\n",
    "This notebook provides a complete example of setting up multimodal RAG with local Ollama models. You can extend this by:\n",
    "\n",
    "- Adding more document types\n",
    "- Implementing conversation memory\n",
    "- Creating a web interface\n",
    "- Fine-tuning the retrieval parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
