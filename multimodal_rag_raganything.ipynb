{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal RAG with RAG-Anything and Ollama\n",
        "\n",
        "This notebook demonstrates how to build a multimodal Retrieval-Augmented Generation (RAG) system using the RAG-Anything library integrated with local Ollama models for chat, embedding, and vision capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install 'raganything[all]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Ollama installed and running locally (download from https://ollama.ai)\n",
        "- Pull required models:\n",
        "  - `ollama pull llama3.2` (for text generation)\n",
        "  - `ollama pull llava` (for vision tasks)\n",
        "  - `ollama pull nomic-embed-text` (for embeddings)\n",
        "- Python 3.8+ with asyncio support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import requests\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from raganything import RAGAnything, RAGAnythingConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama Utility Functions\n",
        "\n",
        "These functions handle communication with the local Ollama server for:\n",
        "- Text chat generation\n",
        "- Text embedding\n",
        "- Vision-based image analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "async def chat_with_ollama(prompt: str, model: str = \"llama3.2\") -> str:\n",
        "    \"\"\"Generate text response using Ollama chat model.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
        "            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"response\"]\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama chat error: {e}\")\n",
        "\n",
        "async def embed_with_ollama(text: str, model: str = \"nomic-embed-text\") -> List[float]:\n",
        "    \"\"\"Generate embeddings for text using Ollama embedding model.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/embeddings\",\n",
        "            json={\"model\": model, \"prompt\": text},\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"embedding\"]\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama embedding error: {e}\")\n",
        "\n",
        "async def vision_with_ollama(image_path: str, prompt: str, model: str = \"qwen3-vl:8b\") -> str:\n",
        "    \"\"\"Analyze image using Ollama vision model.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            image_data = img_file.read()\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"images\": [image_data.hex()],\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"response\"]\n",
        "    except FileNotFoundError:\n",
        "        raise Exception(f\"Image file not found: {image_path}\")\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama vision error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Functions for RAG-Anything\n",
        "\n",
        "Define the required function interfaces that RAG-Anything expects for LLM, vision, and embedding operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def llm_model_func(messages: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"LLM function for RAG-Anything - extracts the last user message and generates response.\"\"\"\n",
        "    user_message = messages[-1][\"content\"] if messages else \"\"\n",
        "    return await chat_with_ollama(user_message)\n",
        "\n",
        "async def vision_model_func(image_path: str, prompt: str) -> str:\n",
        "    \"\"\"Vision function for RAG-Anything - analyzes image with given prompt.\"\"\"\n",
        "    return await vision_with_ollama(image_path, prompt)\n",
        "\n",
        "async def embedding_func(text: str) -> List[float]:\n",
        "    \"\"\"Embedding function for RAG-Anything - generates embeddings for text.\"\"\"\n",
        "    return await embed_with_ollama(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the RAGAnythingConfig with the custom Ollama-based functions and initialize the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure RAG-Anything with Ollama functions\n",
        "config = RAGAnythingConfig(\n",
        "    llm_model_func=llm_model_func,\n",
        "    vision_model_func=vision_model_func,\n",
        "    embedding_func=embedding_func,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    vector_db_path=\"./vector_db\",\n",
        "    enable_vision=True\n",
        ")\n",
        "\n",
        "# Initialize RAG system\n",
        "rag = RAGAnything(config)\n",
        "print(\"RAG-Anything initialized with Ollama integration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Processing\n",
        "\n",
        "Process a sample multimodal document (containing text and potentially images) for the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a sample document (assuming it exists in the docs folder)\n",
        "# This could be a PDF, HTML, or markdown file with embedded images\n",
        "sample_doc_path = \"docs/sample.md\"  # Adjust path as needed\n",
        "\n",
        "try:\n",
        "    await rag.process_document(sample_doc_path)\n",
        "    print(f\"Successfully processed document: {sample_doc_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Document not found: {sample_doc_path}. Please ensure the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing document: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Querying\n",
        "\n",
        "Demonstrate text-based queries and vision-enhanced queries using the multimodal RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Text-based query\n",
        "async def text_query_example():\n",
        "    query = \"What are the main topics covered in the document?\"\n",
        "    try:\n",
        "        result = await rag.query(query)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Response: {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in text query: {e}\")\n",
        "\n",
        "# Example 2: Vision-enhanced query (assuming document contains images)\n",
        "async def vision_query_example():\n",
        "    image_path = \"docs/sample_image.jpg\"  # Adjust path to an actual image\n",
        "    prompt = \"Describe what you see in this image and how it relates to the document content.\"\n",
        "    try:\n",
        "        result = await rag.query_with_vision(prompt, image_path)\n",
        "        print(f\"Vision Query: {prompt}\")\n",
        "        print(f\"Response: {result}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Image not found: {image_path}. Vision query skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in vision query: {e}\")\n",
        "\n",
        "# Run examples\n",
        "await text_query_example()\n",
        "print(\"\\n---\\n\")\n",
        "await vision_query_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal RAG with RAG-Anything and Ollama\n",
        "\n",
        "This notebook demonstrates how to build a multimodal Retrieval-Augmented Generation (RAG) system using the RAG-Anything library integrated with local Ollama models for chat, embedding, and vision capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install rag-anything[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Ollama installed and running locally (download from https://ollama.ai)\n",
        "- Pull required models:\n",
        "  - `ollama pull llama3.2` (for text generation)\n",
        "  - `ollama pull llava` (for vision tasks)\n",
        "  - `ollama pull nomic-embed-text` (for embeddings)\n",
        "- Python 3.8+ with asyncio support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import requests\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from rag_anything import RAGAnything, RAGAnythingConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama Utility Functions\n",
        "\n",
        "These functions handle communication with the local Ollama server for:\n",
        "- Text chat generation\n",
        "- Text embedding\n",
        "- Vision-based image analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "async def chat_with_ollama(prompt: str, model: str = \"llama3.2\") -> str:\n",
        "    \"\"\"Generate text response using Ollama chat model.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
        "            json={\"model\": model, \"prompt\": prompt, \"stream\": False},\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"response\"]\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama chat error: {e}\")\n",
        "\n",
        "async def embed_with_ollama(text: str, model: str = \"nomic-embed-text\") -> List[float]:\n",
        "    \"\"\"Generate embeddings for text using Ollama embedding model.\"\"\"\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/embeddings\",\n",
        "            json={\"model\": model, \"prompt\": text},\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"embedding\"]\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama embedding error: {e}\")\n",
        "\n",
        "async def vision_with_ollama(image_path: str, prompt: str, model: str = \"llava\") -> str:\n",
        "    \"\"\"Analyze image using Ollama vision model.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            image_data = img_file.read()\n",
        "        \n",
        "        response = requests.post(\n",
        "            f\"{OLLAMA_BASE_URL}/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"images\": [image_data.hex()],\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=60\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"response\"]\n",
        "    except FileNotFoundError:\n",
        "        raise Exception(f\"Image file not found: {image_path}\")\n",
        "    except requests.RequestException as e:\n",
        "        raise Exception(f\"Ollama vision error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Functions for RAG-Anything\n",
        "\n",
        "Define the required function interfaces that RAG-Anything expects for LLM, vision, and embedding operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def llm_model_func(messages: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"LLM function for RAG-Anything - extracts the last user message and generates response.\"\"\"\n",
        "    user_message = messages[-1][\"content\"] if messages else \"\"\n",
        "    return await chat_with_ollama(user_message)\n",
        "\n",
        "async def vision_model_func(image_path: str, prompt: str) -> str:\n",
        "    \"\"\"Vision function for RAG-Anything - analyzes image with given prompt.\"\"\"\n",
        "    return await vision_with_ollama(image_path, prompt)\n",
        "\n",
        "async def embedding_func(text: str) -> List[float]:\n",
        "    \"\"\"Embedding function for RAG-Anything - generates embeddings for text.\"\"\"\n",
        "    return await embed_with_ollama(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the RAGAnythingConfig with the custom Ollama-based functions and initialize the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure RAG-Anything with Ollama functions\n",
        "config = RAGAnythingConfig(\n",
        "    llm_model_func=llm_model_func,\n",
        "    vision_model_func=vision_model_func,\n",
        "    embedding_func=embedding_func,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    vector_db_path=\"./vector_db\",\n",
        "    enable_vision=True\n",
        ")\n",
        "\n",
        "# Initialize RAG system\n",
        "rag = RAGAnything(config)\n",
        "print(\"RAG-Anything initialized with Ollama integration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Processing\n",
        "\n",
        "Process a sample multimodal document (containing text and potentially images) for the RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a sample document (assuming it exists in the docs folder)\n",
        "# This could be a PDF, HTML, or markdown file with embedded images\n",
        "sample_doc_path = \"docs/sample.md\"  # Adjust path as needed\n",
        "\n",
        "try:\n",
        "    await rag.process_document(sample_doc_path)\n",
        "    print(f\"Successfully processed document: {sample_doc_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Document not found: {sample_doc_path}. Please ensure the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing document: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Querying\n",
        "\n",
        "Demonstrate text-based queries and vision-enhanced queries using the multimodal RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Text-based query\n",
        "async def text_query_example():\n",
        "    query = \"What are the main topics covered in the document?\"\n",
        "    try:\n",
        "        result = await rag.query(query)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Response: {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in text query: {e}\")\n",
        "\n",
        "# Example 2: Vision-enhanced query (assuming document contains images)\n",
        "async def vision_query_example():\n",
        "    image_path = \"docs/sample_image.jpg\"  # Adjust path to an actual image\n",
        "    prompt = \"Describe what you see in this image and how it relates to the document content.\"\n",
        "    try:\n",
        "        result = await rag.query_with_vision(prompt, image_path)\n",
        "        print(f\"Vision Query: {prompt}\")\n",
        "        print(f\"Response: {result}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Image not found: {image_path}. Vision query skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in vision query: {e}\")\n",
        "\n",
        "# Run examples\n",
        "await text_query_example()\n",
        "print(\"\\n---\\n\")\n",
        "await vision_query_example()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
